{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Imbalance Sampling\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "\n",
    "# Pipelines imports\n",
    "import xgboost\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import GenericUnivariateSelect,f_regression\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Scoring function\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.base import TransformerMixin,BaseEstimator\n",
    "#from scipy import signal\n",
    "\n",
    "# Keras imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing import sequence \n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Bidirectional, MaxPooling1D\n",
    "\n",
    "# Grid Search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4800, 1000)\n",
      "(4800,)\n",
      "(3840, 1000)\n",
      "(3840,)\n"
     ]
    }
   ],
   "source": [
    "# Get data\n",
    "Xdf = pd.read_csv(\"X_train.csv\")\n",
    "XTestdf = pd.read_csv(\"X_test.csv\")\n",
    "ydf = pd.read_csv(\"y_train.csv\")\n",
    "X = Xdf[Xdf.columns[1:]].values\n",
    "Xtest = XTestdf[XTestdf.columns[1:]].values\n",
    "y = ydf[ydf.columns[1]].values\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# Optional scaling\n",
    "scaling=True\n",
    "if scaling:\n",
    "    from sklearn import preprocessing\n",
    "    scaler = preprocessing.RobustScaler().fit(X)\n",
    "    X = scaler.transform(X)\n",
    "    Xtest = scaler.transform(Xtest)\n",
    "\n",
    "# Use these for final training in case of test_train_split\n",
    "X_all = np.copy(X)\n",
    "y_all = np.copy(y)\n",
    "\n",
    "# Split dataset before oversampling to prevent sampling from the \n",
    "split_data = True\n",
    "if split_data:\n",
    "    X, x_val, y, y_val = train_test_split(\n",
    "        X, y, test_size=0.2)\n",
    "\n",
    "# Accounting for imbalance in the data\n",
    "oversample = False\n",
    "undersample = False # Switch both to false to do nothing\n",
    "if oversample:\n",
    "    # Random Oversampler\n",
    "    #ros = RandomOverSampler(random_state=42)\n",
    "    #X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "    \n",
    "    # Smote oversampler\n",
    "    X, y = SMOTE().fit_resample(X, y)\n",
    "    \n",
    "    # Adasyn oversampler\n",
    "    #X_resampled, y_resampled = ADASYN().fit_resample(X, y)\n",
    "elif undersample:\n",
    "    # Count number of occurances per class\n",
    "    _, counts = np.unique(y,return_counts=True)\n",
    "    print(counts)\n",
    "\n",
    "    # Divide by class \n",
    "    X0 = X[y != 1]\n",
    "    X1 = X[y == 1]\n",
    "    y0 = y[y != 1]\n",
    "    y1 = y[y == 1]\n",
    "    print(X0.shape)\n",
    "    print(X1.shape)\n",
    "    print(y0.shape)\n",
    "    print(y1.shape)\n",
    "\n",
    "    # Downsampling\n",
    "    indices_subsampled = np.random.RandomState(seed=42).choice(range(X1.shape[0]),int(counts[0]),replace=False)\n",
    "    X1_subsampled = X1[indices_subsampled]\n",
    "    y1_subsampled = y1[indices_subsampled]\n",
    "    print(X1_subsampled.shape)\n",
    "    print(y1_subsampled.shape)\n",
    "\n",
    "    X = np.concatenate((X0,X1_subsampled))\n",
    "    y = np.concatenate((y0,y1_subsampled))\n",
    "\n",
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.RandomState(seed=42).permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "X,y = unison_shuffled_copies(X,y)\n",
    "\n",
    "one_hot = False\n",
    "if one_hot:\n",
    "    y = np_utils.to_categorical(y)\n",
    "    \n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to get best estimator\n",
    "def get_best_estimator(pipeline, X, y, parameters, scoring, cv=5, verbose=0, n_jobs=None):\n",
    "    print('Finding best parameters through grid search...')\n",
    "    grid = GridSearchCV(pipeline, param_grid=parameters, scoring=scoring, cv=cv, verbose=verbose, refit=True, return_train_score=False, n_jobs=n_jobs)\n",
    "    grid.fit(X, y)\n",
    "    print('Done!')\n",
    "    return grid.best_estimator_, grid\n",
    "\n",
    "def find_best(pipeline, X, y, parameters, scoring, cv=5, verbose=0, n_jobs=None):\n",
    "    best_pipeline, grid = get_best_estimator(pipeline, X, y, parameters, scoring, cv=cv, verbose=verbose, n_jobs=n_jobs)\n",
    "    return grid\n",
    "\n",
    "# If you want to revert 1-hot encoding after classifier in the sklearn pipeline. NOT SURE IF IT WORKS\n",
    "class Revert1Hot(BaseEstimator,TransformerMixin):\n",
    "\n",
    "    # here you define the operation it should perform\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return np.argmax(X)\n",
    "\n",
    "    # just return self\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = make_scorer(balanced_accuracy_score)\n",
    "\n",
    "# Different classifiers that need to be tested\n",
    "#classifiers = [\n",
    "#    KNeighborsClassifier(3),\n",
    "#    SVC(kernel=\"rbf\", C=0.025, probability=True),\n",
    "#    NuSVC(probability=True),\n",
    "#    DecisionTreeClassifier(),\n",
    "#    RandomForestClassifier(),\n",
    "#    AdaBoostClassifier(),\n",
    "#    GradientBoostingClassifier()\n",
    "#    ]\n",
    "\n",
    "steps = [\n",
    "    \n",
    "    ('scaler', StandardScaler()), \n",
    "    ('ufs',GenericUnivariateSelect(score_func=f_regression, mode='k_best', param=200)),\n",
    "    ('XGB', OneVsRestClassifier(xgboost.XGBClassifier(colsample_bytree=0.6,\n",
    "                                                      min_child_weight=6,\n",
    "                                                      max_depth=8)))\n",
    "     ]\n",
    "\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "parameters = {'ufs__param':[1000]}\n",
    "\n",
    "\n",
    "grid = find_best(pipeline, X, y, parameters, score, cv=3, verbose=1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(grid.cv_results_)[['mean_fit_time','mean_score_time','params','mean_test_score','std_test_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still not sure how to extract best model from GridSearch and predict it for the test set\n",
    "pipeline.fit(X,y,scoring=score)\n",
    "result = pipeline.predict(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Not very promising as it only reaches 66.83% accuracy. \n",
    "# Maybe try to use convolutional network although doubting that could help.\n",
    "def create_model():\n",
    "    num_features=1000\n",
    "    mid_size=100\n",
    "    dropout=0.1\n",
    "    \n",
    "    # Model Definition\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_features, activation=\"relu\", input_shape=(num_features,)))\n",
    "    # model.add(Conv1D(filters=64, kernel_size=5, activation='relu', input_shape=(1000,)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(200, activation=\"relu\"))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(50, activation=\"relu\"))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(3, activation=\"softmax\"))\n",
    "\n",
    "    #compile model using accuracy to measure model performance\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'], \n",
    "                  weighted_metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "#Define Early stopping mechanism\n",
    "es = EarlyStopping(monitor='val_loss', mode='max', patience=5, verbose=1)\n",
    "\n",
    "# wrap the model using the function you created   , callbacks=[es]\n",
    "clf = KerasClassifier(build_fn=create_model, epochs=20, batch_size=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=3, shuffle=True)\n",
    "results = cross_val_score(clf, X, y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = make_scorer(balanced_accuracy_score)\n",
    "\n",
    "# Having difficulties adding the tensorflow deep learning model to the sklearn pipeline. \n",
    "# It complains about the keras classifier not having a transform function although it is used in this way in tutorials. \n",
    "# Maybe I have a mistake somewhere.\n",
    "steps = [\n",
    "    \n",
    "    ('scaler', StandardScaler()), \n",
    "    ('ufs',GenericUnivariateSelect(score_func=f_regression, mode='k_best', param=200)),\n",
    "    ('clf', clf),\n",
    "    ('revert1hot', Revert1Hot())\n",
    "     ]\n",
    "\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "#parameters = [{'ufs__param':[200], 'clf__features':[200], 'clf__mid_size':[50, 10]},\n",
    "#             {'ufs__param':[500], 'clf__features':[500], 'clf__mid_size':[100, 20]},\n",
    "#             {'ufs__param':[1000], 'clf__features':[1000], 'clf__mid_size':[200, 100]},]\n",
    "\n",
    "parameters = {'ufs__param':[200], 'clf__features':[200], 'clf__mid_size':[50]}\n",
    "              \n",
    "grid = find_best(pipeline, X, y, parameters, score, cv=5, verbose=1, n_jobs=-1)\n",
    "\n",
    "pandas.DataFrame(grid.cv_results_)[['mean_fit_time','mean_score_time','params','mean_test_score','std_test_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Model Grid search with CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class_weight {0: 6.0, 1: 1.0, 2: 6.0}\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "[CV] batch_size=100, dropout=0.3, epochs=5, init=glorot_uniform, optimizer=adam \n",
      "Model: \"sequential_155\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_481 (Dense)            (None, 128)               128128    \n",
      "_________________________________________________________________\n",
      "dropout_246 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_482 (Dense)            (None, 20)                2580      \n",
      "_________________________________________________________________\n",
      "dense_483 (Dense)            (None, 3)                 63        \n",
      "=================================================================\n",
      "Total params: 130,771\n",
      "Trainable params: 130,771\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " - 1s - loss: 2.0659 - accuracy: 0.5629 - acc: 0.5727\n",
      "Epoch 2/5\n",
      " - 0s - loss: 1.5780 - accuracy: 0.6594 - acc: 0.6857\n",
      "Epoch 3/5\n",
      " - 0s - loss: 1.3802 - accuracy: 0.7094 - acc: 0.7451\n",
      "Epoch 4/5\n",
      " - 0s - loss: 1.1587 - accuracy: 0.7570 - acc: 0.7872\n",
      "Epoch 5/5\n",
      " - 0s - loss: 1.0467 - accuracy: 0.7969 - acc: 0.8234\n",
      "[CV]  batch_size=100, dropout=0.3, epochs=5, init=glorot_uniform, optimizer=adam, total=  21.2s\n",
      "[CV] batch_size=100, dropout=0.3, epochs=5, init=glorot_uniform, optimizer=adam \n",
      "Model: \"sequential_156\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_484 (Dense)            (None, 128)               128128    \n",
      "_________________________________________________________________\n",
      "dropout_247 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_485 (Dense)            (None, 20)                2580      \n",
      "_________________________________________________________________\n",
      "dense_486 (Dense)            (None, 3)                 63        \n",
      "=================================================================\n",
      "Total params: 130,771\n",
      "Trainable params: 130,771\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   21.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " - 1s - loss: 2.1253 - accuracy: 0.5559 - acc: 0.5841\n",
      "Epoch 2/5\n",
      " - 0s - loss: 1.5048 - accuracy: 0.6684 - acc: 0.7106\n",
      "Epoch 3/5\n",
      " - 0s - loss: 1.3580 - accuracy: 0.7195 - acc: 0.7379\n",
      "Epoch 4/5\n",
      " - 0s - loss: 1.1700 - accuracy: 0.7781 - acc: 0.7930\n",
      "Epoch 5/5\n",
      " - 0s - loss: 1.0434 - accuracy: 0.7965 - acc: 0.8126\n",
      "[CV]  batch_size=100, dropout=0.3, epochs=5, init=glorot_uniform, optimizer=adam, total=  21.4s\n",
      "[CV] batch_size=100, dropout=0.3, epochs=5, init=glorot_uniform, optimizer=adam \n",
      "Model: \"sequential_157\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_487 (Dense)            (None, 128)               128128    \n",
      "_________________________________________________________________\n",
      "dropout_248 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_488 (Dense)            (None, 20)                2580      \n",
      "_________________________________________________________________\n",
      "dense_489 (Dense)            (None, 3)                 63        \n",
      "=================================================================\n",
      "Total params: 130,771\n",
      "Trainable params: 130,771\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      " - 2s - loss: 2.0826 - accuracy: 0.5492 - acc: 0.5675\n",
      "Epoch 2/5\n",
      " - 0s - loss: 1.5516 - accuracy: 0.6355 - acc: 0.6806\n",
      "Epoch 3/5\n",
      " - 0s - loss: 1.3233 - accuracy: 0.7043 - acc: 0.7364\n",
      "Epoch 4/5\n",
      " - 0s - loss: 1.1365 - accuracy: 0.7512 - acc: 0.7937\n",
      "Epoch 5/5\n",
      " - 0s - loss: 1.0067 - accuracy: 0.7867 - acc: 0.8269\n",
      "[CV]  batch_size=100, dropout=0.3, epochs=5, init=glorot_uniform, optimizer=adam, total=  22.0s\n",
      "[CV] batch_size=100, dropout=0.3, epochs=15, init=glorot_uniform, optimizer=adam \n",
      "Model: \"sequential_158\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_490 (Dense)            (None, 128)               128128    \n",
      "_________________________________________________________________\n",
      "dropout_249 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_491 (Dense)            (None, 20)                2580      \n",
      "_________________________________________________________________\n",
      "dense_492 (Dense)            (None, 3)                 63        \n",
      "=================================================================\n",
      "Total params: 130,771\n",
      "Trainable params: 130,771\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      " - 1s - loss: 1.9874 - accuracy: 0.5828 - acc: 0.5899\n",
      "Epoch 2/15\n",
      " - 0s - loss: 1.5214 - accuracy: 0.6621 - acc: 0.7031\n",
      "Epoch 3/15\n",
      " - 0s - loss: 1.3408 - accuracy: 0.7191 - acc: 0.7578\n",
      "Epoch 4/15\n",
      " - 0s - loss: 1.2112 - accuracy: 0.7484 - acc: 0.7809\n",
      "Epoch 5/15\n",
      " - 0s - loss: 1.0101 - accuracy: 0.7949 - acc: 0.8285\n",
      "Epoch 6/15\n",
      " - 0s - loss: 0.9134 - accuracy: 0.8145 - acc: 0.8549\n",
      "Epoch 7/15\n",
      " - 0s - loss: 0.7882 - accuracy: 0.8395 - acc: 0.8744\n",
      "Epoch 8/15\n",
      " - 0s - loss: 0.6639 - accuracy: 0.8687 - acc: 0.9043\n",
      "Epoch 9/15\n",
      " - 0s - loss: 0.5809 - accuracy: 0.8766 - acc: 0.9188\n",
      "Epoch 10/15\n",
      " - 0s - loss: 0.5073 - accuracy: 0.8992 - acc: 0.9287\n",
      "Epoch 11/15\n",
      " - 0s - loss: 0.4000 - accuracy: 0.9156 - acc: 0.9435\n",
      "Epoch 12/15\n",
      " - 0s - loss: 0.3403 - accuracy: 0.9266 - acc: 0.9560\n",
      "Epoch 13/15\n",
      " - 0s - loss: 0.3016 - accuracy: 0.9359 - acc: 0.9584\n",
      "Epoch 14/15\n",
      " - 0s - loss: 0.2780 - accuracy: 0.9469 - acc: 0.9657\n",
      "Epoch 15/15\n",
      " - 0s - loss: 0.2456 - accuracy: 0.9520 - acc: 0.9739\n",
      "[CV]  batch_size=100, dropout=0.3, epochs=15, init=glorot_uniform, optimizer=adam, total=  23.9s\n",
      "[CV] batch_size=100, dropout=0.3, epochs=15, init=glorot_uniform, optimizer=adam \n",
      "Model: \"sequential_159\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_493 (Dense)            (None, 128)               128128    \n",
      "_________________________________________________________________\n",
      "dropout_250 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_494 (Dense)            (None, 20)                2580      \n",
      "_________________________________________________________________\n",
      "dense_495 (Dense)            (None, 3)                 63        \n",
      "=================================================================\n",
      "Total params: 130,771\n",
      "Trainable params: 130,771\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      " - 1s - loss: 1.8946 - accuracy: 0.6086 - acc: 0.5867\n",
      "Epoch 2/15\n",
      " - 0s - loss: 1.4008 - accuracy: 0.7133 - acc: 0.7316\n",
      "Epoch 3/15\n",
      " - 0s - loss: 1.1937 - accuracy: 0.7617 - acc: 0.7892\n",
      "Epoch 4/15\n",
      " - 0s - loss: 1.0441 - accuracy: 0.8082 - acc: 0.8222\n",
      "Epoch 5/15\n",
      " - 0s - loss: 0.9053 - accuracy: 0.8145 - acc: 0.8329\n",
      "Epoch 6/15\n",
      " - 0s - loss: 0.8528 - accuracy: 0.8574 - acc: 0.8653\n",
      "Epoch 7/15\n",
      " - 0s - loss: 0.6896 - accuracy: 0.8703 - acc: 0.8929\n",
      "Epoch 8/15\n",
      " - 0s - loss: 0.5917 - accuracy: 0.8969 - acc: 0.9162\n",
      "Epoch 9/15\n",
      " - 0s - loss: 0.4774 - accuracy: 0.9148 - acc: 0.9382\n",
      "Epoch 10/15\n",
      " - 0s - loss: 0.3915 - accuracy: 0.9320 - acc: 0.9512\n",
      "Epoch 11/15\n",
      " - 0s - loss: 0.3660 - accuracy: 0.9398 - acc: 0.9538\n",
      "Epoch 12/15\n",
      " - 0s - loss: 0.2891 - accuracy: 0.9465 - acc: 0.9629\n",
      "Epoch 13/15\n",
      " - 0s - loss: 0.2545 - accuracy: 0.9578 - acc: 0.9724\n",
      "Epoch 14/15\n",
      " - 0s - loss: 0.2059 - accuracy: 0.9594 - acc: 0.9757\n",
      "Epoch 15/15\n",
      " - 0s - loss: 0.1608 - accuracy: 0.9730 - acc: 0.9871\n",
      "[CV]  batch_size=100, dropout=0.3, epochs=15, init=glorot_uniform, optimizer=adam, total=  24.0s\n",
      "[CV] batch_size=100, dropout=0.3, epochs=15, init=glorot_uniform, optimizer=adam \n",
      "Model: \"sequential_160\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_496 (Dense)            (None, 128)               128128    \n",
      "_________________________________________________________________\n",
      "dropout_251 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_497 (Dense)            (None, 20)                2580      \n",
      "_________________________________________________________________\n",
      "dense_498 (Dense)            (None, 3)                 63        \n",
      "=================================================================\n",
      "Total params: 130,771\n",
      "Trainable params: 130,771\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      " - 2s - loss: 1.9460 - accuracy: 0.5988 - acc: 0.5930\n",
      "Epoch 2/15\n",
      " - 0s - loss: 1.5016 - accuracy: 0.7102 - acc: 0.7164\n",
      "Epoch 3/15\n",
      " - 0s - loss: 1.2564 - accuracy: 0.7473 - acc: 0.7729\n",
      "Epoch 4/15\n",
      " - 0s - loss: 1.0875 - accuracy: 0.7898 - acc: 0.8005\n",
      "Epoch 5/15\n",
      " - 0s - loss: 0.9462 - accuracy: 0.8184 - acc: 0.8419\n",
      "Epoch 6/15\n",
      " - 0s - loss: 0.8333 - accuracy: 0.8375 - acc: 0.8660\n",
      "Epoch 7/15\n",
      " - 0s - loss: 0.6821 - accuracy: 0.8723 - acc: 0.8997\n",
      "Epoch 8/15\n",
      " - 0s - loss: 0.5983 - accuracy: 0.8797 - acc: 0.9091\n",
      "Epoch 9/15\n",
      " - 0s - loss: 0.5293 - accuracy: 0.8980 - acc: 0.9242\n",
      "Epoch 10/15\n",
      " - 0s - loss: 0.4537 - accuracy: 0.9086 - acc: 0.9255\n",
      "Epoch 11/15\n",
      " - 0s - loss: 0.4217 - accuracy: 0.9223 - acc: 0.9402\n",
      "Epoch 12/15\n",
      " - 0s - loss: 0.3698 - accuracy: 0.9355 - acc: 0.9513\n",
      "Epoch 13/15\n",
      " - 0s - loss: 0.3040 - accuracy: 0.9391 - acc: 0.9581\n",
      "Epoch 14/15\n",
      " - 0s - loss: 0.2573 - accuracy: 0.9535 - acc: 0.9672\n",
      "Epoch 15/15\n",
      " - 0s - loss: 0.2356 - accuracy: 0.9598 - acc: 0.9752\n",
      "[CV]  batch_size=100, dropout=0.3, epochs=15, init=glorot_uniform, optimizer=adam, total=  24.8s\n",
      "Model: \"sequential_161\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_499 (Dense)            (None, 128)               128128    \n",
      "_________________________________________________________________\n",
      "dropout_252 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_500 (Dense)            (None, 20)                2580      \n",
      "_________________________________________________________________\n",
      "dense_501 (Dense)            (None, 3)                 63        \n",
      "=================================================================\n",
      "Total params: 130,771\n",
      "Trainable params: 130,771\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:  2.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " - 1s - loss: 1.8731 - accuracy: 0.6383 - acc: 0.6141\n",
      "Epoch 2/5\n",
      " - 0s - loss: 1.4580 - accuracy: 0.7182 - acc: 0.7193\n",
      "Epoch 3/5\n",
      " - 0s - loss: 1.3193 - accuracy: 0.7427 - acc: 0.7486\n",
      "Epoch 4/5\n",
      " - 0s - loss: 1.1601 - accuracy: 0.7812 - acc: 0.7905\n",
      "Epoch 5/5\n",
      " - 0s - loss: 0.9878 - accuracy: 0.8159 - acc: 0.8365\n",
      "Best: 0.680455 using {'batch_size': 100, 'dropout': 0.3, 'epochs': 5, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "0.680455 (0.006285) with: {'batch_size': 100, 'dropout': 0.3, 'epochs': 5, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "0.647297 (0.015339) with: {'batch_size': 100, 'dropout': 0.3, 'epochs': 15, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "# Grid Search for parameters in deep learning models\n",
    "# Careful here too. If you use oversample the cross validation might give better results then you would expect\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv1D\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy\n",
    "\n",
    "score = make_scorer(balanced_accuracy_score)\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(optimizer='adam', init='glorot_uniform', dropout=0.2):\n",
    "    num_features=1000\n",
    "  \n",
    "    # Model Definition\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation=\"relu\", kernel_initializer=init, input_shape=(num_features,)))\n",
    "#     model.add(Conv1D(filters=64, kernel_size=5, activation='relu', input_shape=(1000,)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(20, kernel_initializer=init, activation=\"relu\"))\n",
    "    model.add(Dense(3, kernel_initializer=init, activation=\"softmax\"))\n",
    "\n",
    "    # compile model using accuracy to measure model performance, tjese accuracies only show \n",
    "    # accuries of trained data which is too high...\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'], \n",
    "                  weighted_metrics=['acc'])\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=2)\n",
    "\n",
    "# Alternative to over/undersampling, we can also modify the weights in the loss function to give more importance\n",
    "# to underrepresented classes in training. This is cheaper than oversampling and doesn't throw away data\n",
    "# This only makes somewhat sense if you didn't under/oversample the data. Otherwise turn this off\n",
    "if not undersample and not oversample:\n",
    "    class_weight = {0: 6.,\n",
    "                    1: 1.,\n",
    "                    2: 6.}\n",
    "else:\n",
    "    class_weight = {0: 1.,\n",
    "                    1: 1.,\n",
    "                    2: 1.}\n",
    "print(\"Class_weight\", class_weight)\n",
    "# grid search epochs, init, batch size and optimizer\n",
    "optimizers = ['adam']#, 'rmsprop']\n",
    "init = ['glorot_uniform']#, 'normal',]# 'uniform']\n",
    "dropouts = [0.3]\n",
    "epochs = [5, 15]#, 10]#, 100, 150]\n",
    "batches = [100]#, 20]\n",
    "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=init, dropout=dropouts)\n",
    "\n",
    "# set n_jobs to -1 to run on all cores but you lose out on verboseness for some reason\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, verbose=2, scoring=score, n_jobs=1, cv=3)\n",
    "grid_result = grid.fit(X, y, class_weight=class_weight)\n",
    "\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Recreate model with best parameters and export prediction to CSV\n",
    "\n",
    "def create_model(optimizer='adam', init='glorot_uniform', dropout=0.2):\n",
    "    num_features=1000\n",
    "  \n",
    "    # Model Definition\n",
    "    model = Sequential()\n",
    "    regulizer = keras.regularizers.l2(0.000001)\n",
    "    model.add(Dense(num_features, activation=\"relu\", kernel_initializer=init, input_shape=(num_features,), kernel_regularizer=regulizer))\n",
    "    # model.add(Conv1D(filters=64, kernel_size=5, activation='relu', input_shape=(1000,)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(50, kernel_initializer=init, activation=\"relu\", kernel_regularizer=regulizer))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(3, kernel_initializer=init, activation=\"softmax\", kernel_regularizer=regulizer))\n",
    "\n",
    "    #compile model using accuracy to measure model performance\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy')#, metrics=['accuracy'], weighted_metrics=['acc'])\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "# Be careful to use the exact same settings\n",
    "model = KerasClassifier(build_fn=create_model)\n",
    "model.fit(X,Y, epochs=5, batch_size=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "y_test = model.predict(Xtest)\n",
    "df = pd.DataFrame(y_test, columns=[\"y\"])\n",
    "df.index.name = \"id\"\n",
    "df.to_csv(\"results_neural_net_try2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning with cross validation using train_test_split and on the fly cross validation. \n",
    "For this one use test_train split in the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sed/anaconda2/envs/lasif_2.0/lib/python3.6/site-packages/keras/activations.py:235: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3840 samples, validate on 960 samples\n",
      "Epoch 1/30\n",
      " - 3s - loss: 21.3924 - acc: 0.4281 - val_loss: 0.7402 - val_acc: 0.7552\n",
      "Epoch 2/30\n",
      " - 0s - loss: 12.7832 - acc: 0.5771 - val_loss: 1.0393 - val_acc: 0.7573\n",
      "Epoch 3/30\n",
      " - 0s - loss: 9.2766 - acc: 0.6148 - val_loss: 1.0953 - val_acc: 0.7667\n",
      "Epoch 4/30\n",
      " - 0s - loss: 7.5114 - acc: 0.6276 - val_loss: 1.1029 - val_acc: 0.7833\n",
      "Epoch 5/30\n",
      " - 0s - loss: 6.7314 - acc: 0.6365 - val_loss: 1.0685 - val_acc: 0.7781\n",
      "Epoch 6/30\n",
      " - 0s - loss: 5.3106 - acc: 0.6542 - val_loss: 0.9885 - val_acc: 0.7896\n",
      "Epoch 7/30\n",
      " - 0s - loss: 4.8892 - acc: 0.6583 - val_loss: 0.9269 - val_acc: 0.7365\n",
      "Epoch 8/30\n",
      " - 0s - loss: 4.3798 - acc: 0.6529 - val_loss: 0.8786 - val_acc: 0.7229\n",
      "Epoch 9/30\n",
      " - 0s - loss: 3.4970 - acc: 0.6693 - val_loss: 0.8561 - val_acc: 0.6958\n",
      "Epoch 10/30\n",
      " - 0s - loss: 3.2414 - acc: 0.6695 - val_loss: 0.8959 - val_acc: 0.6885\n",
      "Epoch 11/30\n",
      " - 0s - loss: 2.8799 - acc: 0.6727 - val_loss: 0.7943 - val_acc: 0.7042\n",
      "Epoch 12/30\n",
      " - 0s - loss: 2.5422 - acc: 0.6883 - val_loss: 0.6274 - val_acc: 0.7594\n",
      "Epoch 13/30\n",
      " - 0s - loss: 2.3869 - acc: 0.6844 - val_loss: 0.6761 - val_acc: 0.7208\n",
      "Epoch 14/30\n",
      " - 0s - loss: 2.1082 - acc: 0.6805 - val_loss: 0.6204 - val_acc: 0.7563\n",
      "Epoch 15/30\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, GaussianNoise\n",
    "from keras.optimizers import SGD\n",
    "import keras.backend as K\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "# This is tricky when you oversample, because the training data will still look similar to the validation data\n",
    "# x_train, x_val, y_train, y_val = train_test_split(\n",
    "#     X, y, test_size=0.5)\n",
    "# Therefore I moved the splitting part to before the oversampling\n",
    "\n",
    "\n",
    "class Metrics(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self._data = []\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        X_val, y_val = self.validation_data[0], self.validation_data[1]\n",
    "        y_predict = np.asarray(model.predict(X_val))\n",
    "\n",
    "        y_val = np.argmax(y_val, axis=1)\n",
    "        y_predict = np.argmax(y_predict, axis=1)\n",
    "\n",
    "        self._data.append(\n",
    "            balanced_accuracy_score(y_val, y_predict),\n",
    "        )\n",
    "        return\n",
    "\n",
    "    def get_data(self):\n",
    "        return self._data\n",
    "    \n",
    "y_trains = keras.utils.to_categorical(y, num_classes=3)\n",
    "y_tests = keras.utils.to_categorical(y_val, num_classes=3)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "act = keras.layers.LeakyReLU(alpha=0.3)\n",
    "#act = keras.layers.ThresholdedReLU(theta=1.0)\n",
    "#regulizer = keras.regularizers.l1_l2(l1=0.01, l2=0.01)\n",
    "# regulizer = keras.regularizers.l1(0.00001)\n",
    "regulizer = keras.regularizers.l2(0.000001)\n",
    "model.add(Dense(128, activation=\"relu\",kernel_initializer='he_normal', kernel_regularizer=regulizer,input_dim=1000))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(GaussianNoise(12))\n",
    "model.add(Dense(32, kernel_initializer='he_normal',activation=act,kernel_regularizer=regulizer))\n",
    "model.add(GaussianNoise(15))\n",
    "\n",
    "model.add(Dense(16, kernel_initializer='he_normal',activation=act,kernel_regularizer=regulizer))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(3, activation='softmax',kernel_initializer='he_normal'))\n",
    "\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "rmsprop = keras.optimizers.RMSprop(lr=0.001, rho=0.9, decay=0.0)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=rmsprop,\n",
    "              metrics=['acc'])\n",
    "\n",
    "\n",
    "metrics = Metrics()\n",
    "history = model.fit(X, y_trains, validation_data=(x_val, y_tests), epochs=30, batch_size=200, verbose=2,callbacks=[metrics])\n",
    "metrics.get_data()\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(metrics.get_data())\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6860968666720503"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print final cross validation score\n",
    "y_pred = model.predict(x_val)\n",
    "balanced_accuracy_score(y_val, y_pred.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4800 samples, validate on 960 samples\n",
      "Epoch 1/25\n",
      " - 0s - loss: 0.4796 - acc: 0.7900 - val_loss: 0.4904 - val_acc: 0.7865\n",
      "Epoch 2/25\n",
      " - 0s - loss: 0.4603 - acc: 0.7994 - val_loss: 0.4781 - val_acc: 0.7948\n",
      "Epoch 3/25\n",
      " - 0s - loss: 0.4566 - acc: 0.7996 - val_loss: 0.4781 - val_acc: 0.7927\n",
      "Epoch 4/25\n",
      " - 0s - loss: 0.4544 - acc: 0.8140 - val_loss: 0.4607 - val_acc: 0.8188\n",
      "Epoch 5/25\n",
      " - 0s - loss: 0.4420 - acc: 0.8173 - val_loss: 0.4477 - val_acc: 0.8260\n",
      "Epoch 6/25\n",
      " - 0s - loss: 0.4444 - acc: 0.8183 - val_loss: 0.4509 - val_acc: 0.8167\n",
      "Epoch 7/25\n",
      " - 0s - loss: 0.4369 - acc: 0.8190 - val_loss: 0.4435 - val_acc: 0.8229\n",
      "Epoch 8/25\n",
      " - 0s - loss: 0.4279 - acc: 0.8208 - val_loss: 0.4379 - val_acc: 0.8198\n",
      "Epoch 9/25\n",
      " - 0s - loss: 0.4303 - acc: 0.8215 - val_loss: 0.4382 - val_acc: 0.8177\n",
      "Epoch 10/25\n",
      " - 0s - loss: 0.4237 - acc: 0.8252 - val_loss: 0.4278 - val_acc: 0.8271\n",
      "Epoch 11/25\n",
      " - 0s - loss: 0.4161 - acc: 0.8173 - val_loss: 0.4243 - val_acc: 0.8302\n",
      "Epoch 12/25\n",
      " - 0s - loss: 0.4081 - acc: 0.8285 - val_loss: 0.4295 - val_acc: 0.8188\n",
      "Epoch 13/25\n",
      " - 0s - loss: 0.4140 - acc: 0.8346 - val_loss: 0.4270 - val_acc: 0.8125\n",
      "Epoch 14/25\n",
      " - 0s - loss: 0.4127 - acc: 0.8333 - val_loss: 0.4314 - val_acc: 0.8167\n",
      "Epoch 15/25\n",
      " - 0s - loss: 0.4151 - acc: 0.8263 - val_loss: 0.4119 - val_acc: 0.8313\n",
      "Epoch 16/25\n",
      " - 0s - loss: 0.3989 - acc: 0.8275 - val_loss: 0.4156 - val_acc: 0.8156\n",
      "Epoch 17/25\n",
      " - 0s - loss: 0.4017 - acc: 0.8329 - val_loss: 0.3992 - val_acc: 0.8281\n",
      "Epoch 18/25\n",
      " - 0s - loss: 0.4025 - acc: 0.8392 - val_loss: 0.3955 - val_acc: 0.8313\n",
      "Epoch 19/25\n",
      " - 0s - loss: 0.4014 - acc: 0.8354 - val_loss: 0.3914 - val_acc: 0.8365\n",
      "Epoch 20/25\n",
      " - 0s - loss: 0.3973 - acc: 0.8317 - val_loss: 0.3851 - val_acc: 0.8375\n",
      "Epoch 21/25\n",
      " - 0s - loss: 0.4026 - acc: 0.8304 - val_loss: 0.3850 - val_acc: 0.8438\n",
      "Epoch 22/25\n",
      " - 0s - loss: 0.3931 - acc: 0.8383 - val_loss: 0.3784 - val_acc: 0.8417\n",
      "Epoch 23/25\n",
      " - 0s - loss: 0.3876 - acc: 0.8425 - val_loss: 0.3740 - val_acc: 0.8438\n",
      "Epoch 24/25\n",
      " - 0s - loss: 0.3882 - acc: 0.8373 - val_loss: 0.3743 - val_acc: 0.8323\n",
      "Epoch 25/25\n",
      " - 0s - loss: 0.3915 - acc: 0.8435 - val_loss: 0.3805 - val_acc: 0.8323\n"
     ]
    }
   ],
   "source": [
    "# After finding proper settings retrain on the entire dataset\n",
    "# make predictions and rwrite to CSV\n",
    "y_all_cats = keras.utils.to_categorical(y_all, num_classes=3)\n",
    "model.fit(X_all, y_all, validation_data=(x_val, y_tests), epochs=25, batch_size=200, verbose=2,callbacks=[metrics])\n",
    "\n",
    "ypred = model.predict(Xtest)\n",
    "y_test = ypred.argmax(axis=1)\n",
    "\n",
    "# Export to CSV\n",
    "if len(y_test) == 4100: \n",
    "    df = pd.DataFrame(y_test, columns=[\"y\"])\n",
    "    df.index.name = \"id\"\n",
    "    df.to_csv(\"results_neural_net_new_cv2.csv\")\n",
    "else: \n",
    "    raise Exception(\"check length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4800, 3)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not undersample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oversample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([17.60451023, 20.15859119]),\n",
       " 'std_fit_time': array([0.23480723, 0.23574864]),\n",
       " 'mean_score_time': array([3.92661866, 4.10377733]),\n",
       " 'std_score_time': array([0.18482539, 0.18425059]),\n",
       " 'param_batch_size': masked_array(data=[100, 100],\n",
       "              mask=[False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_dropout': masked_array(data=[0.3, 0.3],\n",
       "              mask=[False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_epochs': masked_array(data=[5, 15],\n",
       "              mask=[False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_init': masked_array(data=['glorot_uniform', 'glorot_uniform'],\n",
       "              mask=[False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_optimizer': masked_array(data=['adam', 'adam'],\n",
       "              mask=[False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'batch_size': 100,\n",
       "   'dropout': 0.3,\n",
       "   'epochs': 5,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'adam'},\n",
       "  {'batch_size': 100,\n",
       "   'dropout': 0.3,\n",
       "   'epochs': 15,\n",
       "   'init': 'glorot_uniform',\n",
       "   'optimizer': 'adam'}],\n",
       " 'split0_test_score': array([0.67734024, 0.65382812]),\n",
       " 'split1_test_score': array([0.67480414, 0.6261168 ]),\n",
       " 'split2_test_score': array([0.68922152, 0.66194653]),\n",
       " 'mean_test_score': array([0.6804553 , 0.64729715]),\n",
       " 'std_test_score': array([0.00628453, 0.01533911]),\n",
       " 'rank_test_score': array([1, 2], dtype=int32)}"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_result.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
